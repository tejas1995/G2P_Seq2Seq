{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grapheme2Phoneme Translation with an Encoder-Decoder Model\n",
    "\n",
    "Grapheme-to-phoneme (G2P) is used to convert words (sequences of grapheme's in a specific language's script) into sequences of IPA units. In this work, we build a G2P model that is trained on and can be applied to a large number of languages (even low-resource ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "We start by importing some libraries (including pyTorch v1.4.0), and defining some variables and hyperparameters which will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "BOS = \"<BOS>\"\n",
    "EOS = \"<EOS>\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-3\n",
    "L2_NORM = 1e-5\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "EPOCHS = 10\n",
    "BEAM_WIDTH = 5\n",
    "checkpt_filename = 'saved_models/seq2seq-{}layer.ckpt'.format(NUM_LAYERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading the provided data. The important elements for each data point are the grapheme sequence, phoneme sequence, and language (stored as iso-script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    f = open(filename)\n",
    "    data = [x.strip('\\n').split('\\t') for x in f.readlines()]\n",
    "    data = [[list(x[2]), x[3].split(), \"-\".join([x[0], x[1]])] for x in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'pron_data/gold_data_train'\n",
    "test_filename = 'pron_data/gold_data_test'\n",
    "\n",
    "train_data = load_data(train_filename)\n",
    "test_data = load_data(test_filename)\n",
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build our vocabularies for graphemes and phonemes, as well as a list of languages. These language tokens will be concatenated to the grapheme sequence, and are thus included in the grapheme vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of languages: 420\n",
      "Size of grapheme vocab: 4690\n",
      "Size of phoneme vocab: 569\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "grapheme_freq_dict = defaultdict(int)\n",
    "phoneme_freq_dict = defaultdict(int)\n",
    "\n",
    "for d in train_data+test_data:\n",
    "    for g in d[0]:\n",
    "        grapheme_freq_dict[g] += 1\n",
    "    for p in d[1]:\n",
    "        phoneme_freq_dict[p] += 1\n",
    "\n",
    "langs_list_train = []\n",
    "for d in train_data:\n",
    "    langs_list_train.append(d[2])\n",
    "langs_list_train = sorted(list(set(langs_list_train)))\n",
    "    \n",
    "grapheme_vocab = [PAD, UNK, BOS, EOS] + sorted(list(grapheme_freq_dict.keys()))\n",
    "phoneme_vocab = [PAD, UNK, BOS, EOS] + sorted(list(phoneme_freq_dict.keys()))\n",
    "\n",
    "langs_list = langs_list_train\n",
    "print(\"Number of languages: {}\".format(len(langs_list)))\n",
    "grapheme_vocab += langs_list\n",
    "\n",
    "print(\"Size of grapheme vocab: {}\".format(len(grapheme_vocab)))\n",
    "print(\"Size of phoneme vocab: {}\".format(len(phoneme_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our G2PDataset class. Since the testing data might contain languages which are not present in the training data, we replace their language tokens with UNK. The dataset yields batches with grapheme and phoneme sequences, as well as the lengths of the grapheme sequence. \n",
    "\n",
    "The grapheme sequence has the language token prepended - this resulted in significant performance gains (~5% PER improvement on the validation set, ~10% PER improvement on the test set). An example input sequence would thus be:\n",
    "`eng-Latin` `z` `e` `r` `o`\n",
    "\n",
    "We use 10% of our training data as the validation set, while the remainder is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, g2p_data, grapheme_vocab, phoneme_vocab):\n",
    "        self.data = g2p_data\n",
    "        self.grapheme_vocab = grapheme_vocab\n",
    "        self.phoneme_vocab = phoneme_vocab\n",
    "        for i in range(len(self.data)):\n",
    "            if self.data[i][2] not in grapheme_vocab:\n",
    "                self.data[i][2] = UNK\n",
    "        self.g2i = {g: grapheme_vocab.index(g) for g in grapheme_vocab}\n",
    "        self.p2i = {p: phoneme_vocab.index(p) for p in phoneme_vocab}\n",
    "        self.i2p = {phoneme_vocab.index(p): p for p in phoneme_vocab}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'G': torch.tensor([self.g2i[self.data[index][2]]] + [self.g2i[g] for g in self.data[index][0]]), \n",
    "                'P': torch.tensor([self.p2i[p] for p in ([BOS]+self.data[index][1]+[EOS])]),\n",
    "                'G_len': len(self.data[index][0]),\n",
    "                'lang': self.data[index][2]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_data_size = int(0.9*len(train_data))\n",
    "train_dataset = G2PDataset(train_data[:train_data_size], grapheme_vocab, phoneme_vocab)\n",
    "val_dataset = G2PDataset(train_data[train_data_size:], grapheme_vocab, phoneme_vocab)\n",
    "test_dataset = G2PDataset(test_data, grapheme_vocab, phoneme_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define iterators for each of our datasets. The collate function handles the padding using the PAD token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence_batch(batch, dataset):\n",
    "    g_seqs = [x['G'] for x in batch]\n",
    "    p_seqs = [x['P'] for x in batch]\n",
    "    g_lens = [x['G_len'] for x in batch]\n",
    "    langs = [x['lang'] for x in batch]\n",
    "    return {'G': pad_sequence(g_seqs, batch_first=True, padding_value=dataset.g2i[PAD]),\n",
    "            'P': pad_sequence(p_seqs, batch_first=True, padding_value=dataset.p2i[PAD]), \n",
    "            'G_lens': g_lens, \n",
    "            'langs': langs}\n",
    "\n",
    "train_iterator = DataLoader(train_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True, \n",
    "                            collate_fn=partial(pad_sequence_batch, dataset = train_dataset))\n",
    "\n",
    "val_iterator = DataLoader(val_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, \n",
    "                          collate_fn=partial(pad_sequence_batch, dataset = train_dataset))\n",
    "\n",
    "test_iterator = DataLoader(test_dataset,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           shuffle=False, \n",
    "                           collate_fn=partial(pad_sequence_batch, dataset = train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is a Seq2Seq/Encoder-Decoder model. Encoder-Decoder models are standard for similar sequence-to-sequence tasks like Machine Translations. We use RNNs in our Encoders and Decoders instead of Transformers, since these are generally less parameter-intensive.\n",
    "\n",
    "The Encoder is a 2-layer LSTM, with 300-dim embeddings and 256-dim hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_size, hid_size, num_layers, grapheme_vocab):\n",
    "        super(Encoder, self).__init__() \n",
    "        vocab_size = len(grapheme_vocab)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=grapheme_vocab.index(PAD))\n",
    "        self.encoder = nn.LSTM(emb_size, hid_size, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, seqs, lens):\n",
    "        \n",
    "        # Extract grapheme embeddings\n",
    "        embs = self.embedding(seqs)\n",
    "        \n",
    "        # Before passing through the LSTM, we pack the padded embeddings \n",
    "        sort_idx = sorted(range(len(lens)), key=lambda i: -lens[i])\n",
    "        sorted_embs = embs[sort_idx]\n",
    "        sorted_lens = [lens[i] for i in sort_idx]\n",
    "        packed = pack_padded_sequence(sorted_embs, sorted_lens, batch_first=True)\n",
    "        sorted_outputs, sorted_hidden = self.encoder(packed)\n",
    "        sorted_outputs, _ = pad_packed_sequence(sorted_outputs, batch_first=True)\n",
    "        unsort_idx = sorted(range(len(lens)), key=lambda i: sort_idx[i])\n",
    "        outputs = sorted_outputs[unsort_idx]\n",
    "        hidden = (sorted_hidden[0][:,unsort_idx], sorted_hidden[1][:,unsort_idx])\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is a 2-layer GRU, with dropout after the phoneme embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, emb_size, hid_size, num_layers, phoneme_vocab, dropout_p=0.3):\n",
    "        super(Decoder, self).__init__()\n",
    "        vocab_size = len(phoneme_vocab)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.decoder = nn.GRU(emb_size, hid_size, num_layers)\n",
    "        self.out = nn.Linear(hid_size, vocab_size)\n",
    "        \n",
    "    def forward(self, hidden, last_word, encoder_outputs):\n",
    "        emb = self.embedding(last_word).unsqueeze(0)\n",
    "        emb = self.dropout(emb)\n",
    "        output, hidden = self.decoder(emb, hidden)\n",
    "        return F.log_softmax(self.out(output), dim=2), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a decoder which uses attention (Bahdanau et al., 2015); however, our primary results are obtained without attention since the benefits of attention for this task were very marginal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, emb_size, hid_size, num_layers, phoneme_vocab, dropout_p=0.3):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        vocab_size = len(phoneme_vocab)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.attn_Wa = nn.Linear(hid_size*2, hid_size)\n",
    "        self.attn_v = nn.Linear(hid_size, 1)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.decoder = nn.GRU(emb_size+hid_size, hid_size, num_layers)\n",
    "        self.out = nn.Linear(hid_size, vocab_size)\n",
    "        \n",
    "    def forward(self, hidden, last_word, encoder_outputs):\n",
    "        emb = self.embedding(last_word).unsqueeze(0)\n",
    "        \n",
    "        h = hidden[-1].unsqueeze(0).permute(1,0,2).repeat(1, encoder_outputs.shape[1], 1)\n",
    "        attn_energies = torch.tanh(self.attn_Wa(torch.cat((h, encoder_outputs), dim=2)))\n",
    "        attn_logits = self.attn_v(attn_energies)\n",
    "        attn_weights = F.softmax(attn_logits, dim=1)\n",
    "        context_vec = torch.bmm(attn_weights.permute(0,2,1), encoder_outputs).permute(1,0,2)\n",
    "\n",
    "        emb = torch.cat((emb, context_vec), dim=2)\n",
    "        emb = self.dropout(emb)\n",
    "        output, hidden = self.decoder(emb, hidden)\n",
    "        return F.log_softmax(self.out(output), dim=2), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our overall Seq2Seq model thus consists of our above defined Encoder and Decoder. We also define two methods for testing - `decode()` performs greedy decoding, whereas `beam_decode()` performs beam search decoding. The implementation of `beam_decode()` operates on a single grapheme sequence rather than a batch - batching the beam search would result in significant speed improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, emb_size, hid_size, num_layers, grapheme_vocab, phoneme_vocab, max_len=30):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(emb_size, hid_size, num_layers, grapheme_vocab)\n",
    "        self.decoder = Decoder(emb_size, hid_size, num_layers, phoneme_vocab)\n",
    "        #self.decoder = AttnDecoder(emb_size, hid_size, num_layers, phoneme_vocab)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def forward(self, grapheme_seq, phoneme_seq, g_lens):\n",
    "        \n",
    "        #print(\"phoneme seq:\", phoneme_seq.shape)\n",
    "        enc_outputs, enc_hidden = self.encoder(grapheme_seq, g_lens)\n",
    "        dec_hidden = enc_hidden[1]\n",
    "        \n",
    "        probs = torch.zeros(phoneme_seq.shape[0], phoneme_seq.shape[1], len(phoneme_vocab))\n",
    "        #print(\"probs shape:\", probs.shape)\n",
    "        last_word = phoneme_seq[:,0]\n",
    "        for t in range(1, phoneme_seq.shape[1]):\n",
    "            dec_output, dec_hidden = self.decoder(dec_hidden, last_word, enc_outputs)\n",
    "            probs[:,t,:] = dec_output.squeeze(0)\n",
    "            last_word = phoneme_seq[:,t]\n",
    "\n",
    "        return probs\n",
    "    \n",
    "    def decode(self, grapheme_seq, g_lens):\n",
    "        \n",
    "        batch_size = len(g_lens)\n",
    "        enc_outputs, enc_hidden = self.encoder(grapheme_seq, g_lens)\n",
    "        dec_hidden = enc_hidden[1]\n",
    "        \n",
    "        predictions = torch.zeros(grapheme_seq.shape[0], self.max_len)\n",
    "        last_word = torch.LongTensor([phoneme_vocab.index(BOS) for i in range(batch_size)]).to(device)\n",
    "        for t in range(1, self.max_len):\n",
    "            dec_output, dec_hidden = self.decoder(dec_hidden, last_word, enc_outputs)\n",
    "            \n",
    "            topv, topi = dec_output.data.topk(1)\n",
    "            topi = topi.view(-1)\n",
    "            predictions[:, t] = topi\n",
    "            last_word = topi.detach()\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "    def beam_decode(self, grapheme_seq, g_lens, beam_width):\n",
    "        # batch size must be 1\n",
    "        batch_size = len(g_lens)\n",
    "        grapheme_seq = grapheme_seq.unsqueeze(0)\n",
    "\n",
    "        enc_outputs, enc_hidden = self.encoder(grapheme_seq, g_lens)\n",
    "        dec_hidden = enc_hidden[1]\n",
    "        \n",
    "        last_word = torch.LongTensor([phoneme_vocab.index(BOS)]).to(device)\n",
    "        init_hyp = {'last_word': last_word, \n",
    "                    'log_p': 0.0, \n",
    "                    'length': 0, \n",
    "                    'seq': [phoneme_vocab.index(BOS)], \n",
    "                    'dec_hidden': dec_hidden}\n",
    "        beam = [init_hyp]\n",
    "        for t in range(1, self.max_len):\n",
    "            new_beam = []\n",
    "            for hyp in beam:\n",
    "\n",
    "                if hyp['seq'][-1] == phoneme_vocab.index(EOS):\n",
    "                    new_beam.append(hyp)\n",
    "                    continue\n",
    "                \n",
    "                dec_output, dec_hidden = self.decoder(hyp['dec_hidden'], hyp['last_word'], enc_outputs)\n",
    "                topv, topi = dec_output.data.topk(beam_width)\n",
    "                topi = topi.view(-1)\n",
    "                topv = topv.view(-1)\n",
    "                for i in range(beam_width):\n",
    "                    last_word = torch.LongTensor([topi[i].detach()]).to(device)\n",
    "                    new_hyp = {'last_word': last_word, \n",
    "                               'log_p': hyp['log_p']+topv[i].item(), \n",
    "                               'length': hyp['length']+1, \n",
    "                               'seq': hyp['seq']+[topi[i].item()], \n",
    "                               'dec_hidden': dec_hidden}\n",
    "                    new_beam.append(new_hyp)\n",
    "\n",
    "            beam = sorted(new_beam, \n",
    "                          key=lambda x: x['log_p']/x['length'], \n",
    "                          reverse=True)\n",
    "            beam = beam[:beam_width]\n",
    "            \n",
    "        return [h['seq'] for h in beam]\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two methods to turn our decoding outputs into a string sequence of phonemes. We also define a method for our evaluation metric - Phoneme Error Rate (PER). PER is equal to the sum of Levenshtein edit distance of all samples in the test set, divided by the sum of reference phoneme sequence lengths. This metric is similar to Word Error Rate/Character Error Rate which are used to evaluate speech recognition systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def get_sequences(out_tensor, i2p):\n",
    "    out_seqs = []\n",
    "    for i in range(out_tensor.shape[0]):\n",
    "        seq = []\n",
    "        for t in range(out_tensor.shape[1]):\n",
    "            p = i2p[out_tensor[i][t].item()]\n",
    "            if p == EOS:\n",
    "                break\n",
    "            seq.append(p)\n",
    "        out_seqs.append(''.join(seq[1:]))\n",
    "    return out_seqs\n",
    "\n",
    "def get_beam_sequences(out_matrix, i2p):\n",
    "    out_seqs = []\n",
    "    for i in range(len(out_matrix)):\n",
    "        seq = []\n",
    "        for t in range(len(out_matrix[i])):\n",
    "            p = i2p[out_matrix[i][t]]\n",
    "            if p == EOS:\n",
    "                break\n",
    "            seq.append(p)\n",
    "        out_seqs.append(''.join(seq[1:]))\n",
    "    return out_seqs\n",
    "\n",
    "def phoneme_error_rate(predicted_seqs, reference_seqs):\n",
    "    num_samples = len(predicted_seqs)\n",
    "    ref_len = 0\n",
    "    edit_distance = 0\n",
    "    for pred, ref in zip(predicted_seqs, reference_seqs):\n",
    "        edit_distance += Levenshtein.distance(''.join(pred), ''.join(ref))\n",
    "        ref_len += len(ref)\n",
    "        \n",
    "    return 100.0*edit_distance/ref_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which create an instance of our Seq2Seq model using the hyperparameters declared in the beginning, as well as a loss criterion and optimizer. We use weight decay for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsriniva/anaconda2/envs/ethics_env/lib/python3.8/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "#model = Encoder(100, 100, grapheme_vocab)\n",
    "model = Seq2Seq(EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, grapheme_vocab, phoneme_vocab)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index=phoneme_vocab.index(PAD), size_average=True)\n",
    "optimizer = optim.Adam(lr=LR, params=model.parameters(), weight_decay=L2_NORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the model, we train it! After each epoch, we compute the validation PER (using greedy decoding), and save the model checkpoint if the validation PER is better (lower) than previous epoch checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0: 100%|██████████| 17771/17771 [11:39<00:00, 25.40it/s]\n",
      "100%|██████████| 1975/1975 [00:40<00:00, 48.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Val PER = 20.40%\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 17771/17771 [11:41<00:00, 25.34it/s]\n",
      "100%|██████████| 1975/1975 [00:40<00:00, 48.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val PER = 17.50%\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 17771/17771 [11:41<00:00, 25.33it/s]\n",
      "100%|██████████| 1975/1975 [00:40<00:00, 48.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Val PER = 16.59%\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3: 100%|██████████| 17771/17771 [11:40<00:00, 25.38it/s]\n",
      "100%|██████████| 1975/1975 [00:40<00:00, 48.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Val PER = 15.95%\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4: 100%|██████████| 17771/17771 [11:40<00:00, 25.35it/s]\n",
      "100%|██████████| 1975/1975 [00:40<00:00, 48.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Val PER = 15.59%\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5: 100%|██████████| 17771/17771 [11:41<00:00, 25.34it/s]\n",
      "100%|██████████| 1975/1975 [00:40<00:00, 48.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Val PER = 15.47%\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 6: 100%|██████████| 17771/17771 [11:42<00:00, 25.30it/s]\n",
      "100%|██████████| 1975/1975 [00:40<00:00, 48.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Val PER = 15.15%\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 7: 100%|██████████| 17771/17771 [11:41<00:00, 25.34it/s]\n",
      "100%|██████████| 1975/1975 [00:40<00:00, 48.47it/s]\n",
      "Training epoch 8:   0%|          | 3/17771 [00:00<11:32, 25.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Val PER = 15.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 8: 100%|██████████| 17771/17771 [11:41<00:00, 25.33it/s]\n",
      "100%|██████████| 1975/1975 [00:39<00:00, 49.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Val PER = 14.93%\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 9: 100%|██████████| 17771/17771 [11:37<00:00, 25.49it/s]\n",
      "100%|██████████| 1975/1975 [00:38<00:00, 51.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Val PER = 14.92%\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "losses_list = []\n",
    "best_per = 100.0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_iterator, desc=\"Training epoch {}\".format(epoch)):\n",
    "        optimizer.zero_grad()\n",
    "        g = batch['G'].to(device)\n",
    "        p = batch['P'].to(device)\n",
    "        g_lens = batch['G_lens']\n",
    "\n",
    "        decode_probs = model(g, p, g_lens).to(device)\n",
    "\n",
    "        loss = criterion(decode_probs.view(-1, decode_probs.size(-1)), p.flatten())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses_list.append(loss.item())\n",
    "        \n",
    "    model.eval()\n",
    "    predicted_phoneme_seqs = []\n",
    "    reference_phoneme_seqs = []\n",
    "    for batch in tqdm(val_iterator):\n",
    "        optimizer.zero_grad()\n",
    "        g = batch['G'].to(device)\n",
    "        p = batch['P'].to(device)\n",
    "        g_lens = batch['G_lens']\n",
    "\n",
    "        preds = model.decode(g, g_lens)\n",
    "\n",
    "        pred_seqs = get_sequences(preds, test_dataset.i2p)\n",
    "        ref_seqs = get_sequences(p, test_dataset.i2p)\n",
    "        predicted_phoneme_seqs.extend(pred_seqs)\n",
    "        reference_phoneme_seqs.extend(ref_seqs)\n",
    "    per = phoneme_error_rate(predicted_phoneme_seqs, reference_phoneme_seqs)\n",
    "    print(\"Epoch {}: Val PER = {:.2f}%\".format(epoch, per))\n",
    "    if per < best_per:\n",
    "        print(\"Saving model...\")\n",
    "        torch.save(model.state_dict(), checkpt_filename)\n",
    "        best_per = per\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate our model on the provided test set using both greedy and beam search decoding. We see that beam search doesn't result in much improvements in terms of PER, but takes significantly longer (10 minutes, compared to 15 seconds for greedy decoding) - however, it does provide multiple hypotheses with probabilities (although only the top hypothesis is used for evaluation). An alternative method to get multiple hypotheses would be *nucleus sampling*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_iterator, beam_decode=False):\n",
    "    model.eval()\n",
    "    predicted_phoneme_seqs = []\n",
    "    reference_phoneme_seqs = []\n",
    "    model.load_state_dict(torch.load(checkpt_filename))\n",
    "    for batch in tqdm(data_iterator):\n",
    "        g = batch['G'].to(device)\n",
    "        p = batch['P'].to(device)\n",
    "        g_lens = batch['G_lens']\n",
    "        langs = batch['langs']\n",
    "\n",
    "        if beam_decode is True:\n",
    "            pred_seqs = []\n",
    "            for j in range(len(g_lens)):\n",
    "                hyps = model.beam_decode(g[j], [g_lens[j]], BEAM_WIDTH)\n",
    "                pred_seqs.extend(get_beam_sequences([hyps[0]], test_dataset.i2p))\n",
    "        else:\n",
    "            preds = model.decode(g, g_lens)\n",
    "            pred_seqs = get_sequences(preds, test_dataset.i2p)\n",
    "\n",
    "        ref_seqs = get_sequences(p, test_dataset.i2p)\n",
    "\n",
    "        predicted_phoneme_seqs.extend(pred_seqs)\n",
    "        reference_phoneme_seqs.extend(ref_seqs)\n",
    "\n",
    "    #print(\"Number of samples: {}\".format(len(predicted_phoneme_seqs)))\n",
    "    per = phoneme_error_rate(predicted_phoneme_seqs, reference_phoneme_seqs)\n",
    "    #print(\"PER = {:.2f}%\".format(per))\n",
    "    return per, predicted_phoneme_seqs, reference_phoneme_seqs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:14<00:00, 57.37it/s]\n",
      "  0%|          | 0/810 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PER for greedy search: 38.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [09:26<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PER for beam search: 39.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_per_greedy, predicted_phoneme_seqs, reference_phoneme_seqs  = evaluate_model(model, test_iterator, beam_decode=False)\n",
    "print(\"Test PER for greedy search: {:.2f}%\".format(test_per_greedy))\n",
    "\n",
    "test_per_beam, _, _ = evaluate_model(model, test_iterator, beam_decode=True)\n",
    "print(\"Test PER for beam search: {:.2f}%\".format(test_per_beam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PER for a testing dataset of more than 500 languages lies at around 39%. These results are comparable to [this paper](https://www.aclweb.org/anthology/P16-1038.pdf) (Table 8), which also does grapheme-to-phoneme conversion for a large set of languages.\n",
    "\n",
    "We see that beam search doesn't result in much improvements in terms of PER, but takes significantly longer (10 minutes, compared to 15 seconds for greedy decoding) - however, it does provide multiple hypotheses with probabilities (although only the top hypothesis is used for evaluation). An alternative method to get multiple hypotheses would be *nucleus sampling*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of predicted and reference phoneme sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kakwə kɑkwəʔ\n",
      "kaksu ħʌɖʌ\n",
      "laɡo laɡo\n",
      "ni ni\n",
      "ha hu\n",
      "huɔt huok\n",
      "kiɔn kioɾ\n",
      "naju nɑis\n",
      "nweju nueik\n",
      "nwɔt nuop\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(predicted_phoneme_seqs[i], reference_phoneme_seqs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3644957\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Number of parameters: {}\".format(total_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### Comparison of Network Complexity\n",
    "\n",
    "The above model has 2 layers in both the encoder and decoder. We experiment with different number of layers to investigate the performance-efficiency tradeoff - fewer layers will result in smaller models and faster decoding, but might lead to performance drop.\n",
    "\n",
    "| # layers | #params | Test PER  (greedy) | Val PER (greedy) | Training time per epoch | Test decoding time (beam) |\n",
    "|----------|---------|--------------------|------------------|--------------------------|---------------------------|\n",
    "| 1        | 2.7M    |  39.97%            |    15.94%        |  10 mins                 |    9 mins              |\n",
    "| 2        | 3.6M    |  38.83%            |    14.92%        |  12 mins                 |    9.5 mins            |\n",
    "| 3        | 4.5M    |  37.82%            |    14.39%        |  14 mins                 |    10 mins             |\n",
    "\n",
    "We see that the models with more layers have lower validation and testing PER, but take slightly longer to both train and decode. We see that with a 20% reduction in parameters, our test PER drops by only 1%.\n",
    "\n",
    "### Seq2Seq with Attention\n",
    "\n",
    "| Attention | #params | Test PER  (greedy) | Val PER  (greedy) | Training time  per epoch | Test decoding time (beam) |\n",
    "|-----------|---------|--------------------|-------------------|--------------------------|--------------------------|\n",
    "| No        | 3.6M    | 38.83%             | 14.92%            | 12 mins                  | 9.5 mins                 |\n",
    "| Yes       | 4.0M    | 38.23%             | 14.76%            | 17 mins                  | 13 mins                  |\n",
    "\n",
    "We see that attention results in longer training and testing times, as well as a 10% increase in parameteres, which minimal PER improvements.\n",
    "\n",
    "### Disparity between Validation and Test Performance\n",
    "\n",
    "There is a large different between the PER over the validation and test set. This suggests different data distributions in our training set (which our val set is a subset of) and the testing data. One factor could be languages present in the test set which weren't seen during training: indeed, ~4% of test samples (1k out of 25k) belong to a language which is not represented in the training data. \n",
    "\n",
    "Another reason is overfitting. We have a regularization term (L2 norm): however, experiments with different hyperparameters did not yield improvements on the test score\n",
    "\n",
    "Another possibility is that it performs differently on high-resource and low-resource data in the two datasets. Analyzing the performance on top-50 and other languages, we see that our model performs poorly on low-resource languages in both datasets, but the test set has more low-resource languages than the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1975 [00:00<00:38, 51.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1975/1975 [00:37<00:00, 52.33it/s]\n",
      "  0%|          | 0/810 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of samples in top languages: 97.57%\n",
      "Top languages PER = 14.34%\n",
      "Other languages PER = 50.42%\n",
      "-----------------------\n",
      "Testing:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:14<00:00, 57.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of samples in top languages: 35.47%\n",
      "Top languages PER = 22.49%\n",
      "Other languages PER = 50.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lang_freq = defaultdict(int)\n",
    "for d in train_data:\n",
    "    lang_freq[d[2]] += 1\n",
    "top_50_langs = sorted(list(lang_freq.keys()), key=lambda x: lang_freq[x], reverse=True)[:50]\n",
    "\n",
    "\n",
    "def evaluate_model_langwise(model, data_iterator, langs_list, beam_decode=False):\n",
    "    model.eval()\n",
    "    predicted_seqs_toplangs = []\n",
    "    reference_seqs_toplangs = []\n",
    "    predicted_seqs_otherlangs = []\n",
    "    reference_seqs_otherlangs = []\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpt_filename))\n",
    "    for batch in tqdm(data_iterator):\n",
    "        g = batch['G'].to(device)\n",
    "        p = batch['P'].to(device)\n",
    "        g_lens = batch['G_lens']\n",
    "        langs = batch['langs']\n",
    "\n",
    "        preds = model.decode(g, g_lens)\n",
    "        pred_seqs = get_sequences(preds, test_dataset.i2p)\n",
    "        ref_seqs = get_sequences(p, test_dataset.i2p)\n",
    "\n",
    "        for i, l in enumerate(langs):\n",
    "            if l in langs_list:\n",
    "                predicted_seqs_toplangs.append(pred_seqs[i])\n",
    "                reference_seqs_toplangs.append(ref_seqs[i])\n",
    "            else:\n",
    "                predicted_seqs_otherlangs.append(pred_seqs[i])\n",
    "                reference_seqs_otherlangs.append(ref_seqs[i])\n",
    "\n",
    "\n",
    "    print(\"% of samples in top languages: {:.2f}%\".format(100.0*len(predicted_seqs_toplangs)/(len(predicted_seqs_toplangs)+len(predicted_seqs_otherlangs))))\n",
    "    toplangs_per = phoneme_error_rate(predicted_seqs_toplangs, reference_seqs_toplangs)\n",
    "    otherlangs_per = phoneme_error_rate(predicted_seqs_otherlangs, reference_seqs_otherlangs)\n",
    "    print(\"Top languages PER = {:.2f}%\".format(toplangs_per))\n",
    "    print(\"Other languages PER = {:.2f}%\".format(otherlangs_per))\n",
    "    \n",
    "print(\"Validation:\")\n",
    "evaluate_model_langwise(model, val_iterator, top_50_langs)\n",
    "print('-----------------------')\n",
    "print('Testing:')\n",
    "evaluate_model_langwise(model, test_iterator, top_50_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
